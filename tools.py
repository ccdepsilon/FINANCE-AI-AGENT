import pandas as pd
import numpy as np
import akshare as ak
import os
from datetime import datetime, timedelta

# å®šä¹‰è¾“å‡ºç›®å½•
OUTPUT_DIR = "./output"

class DataProcessor:
    """
    ç»„å‘˜ Aï¼šåŸºç¡€æ•°æ®å¤„ç†ç±» (AkShare å®æˆ˜ç‰ˆ)
    """

    @staticmethod
    def normalize_akshare_data(df_raw: pd.DataFrame) -> pd.DataFrame:
        """
        [å…³é”®é€‚é…å™¨] å°† AkShare çš„ç¾è‚¡æ•°æ®æ ¼å¼æ ‡å‡†åŒ–ã€‚
        AkShare ç¾è‚¡æ¥å£è¿”å›çš„åˆ—åé€šå¸¸æ˜¯å°å†™ (date, open, close)ï¼Œæˆ‘ä»¬éœ€è¦è½¬ä¸ºé¦–å­—æ¯å¤§å†™ã€‚
        """
        df = df_raw.copy()
        
        # 1. å®šä¹‰åˆ—åæ˜ å°„å­—å…¸ (AkShare US -> Standard)
        # å³ä½¿ AkShare æœªæ¥å˜äº†ï¼Œæˆ‘ä»¬ä¹Ÿåªéœ€è¦æ”¹è¿™é‡Œ
        rename_map = {
            'date': 'Date', 
            'open': 'Open', 
            'high': 'High', 
            'low': 'Low', 
            'close': 'Close', 
            'volume': 'Volume',
            'adjusted_close': 'Adj Close' 
        }
        
        # 2. é‡å‘½å
        df.rename(columns=rename_map, inplace=True)
        
        # 3. ç¡®ä¿å¿…é¡»çš„åˆ—å­˜åœ¨
        required = ['Date', 'Close']
        for col in required:
            if col not in df.columns:
                raise ValueError(f"æ•°æ®å¼‚å¸¸ï¼šAkShare è¿”å›çš„æ•°æ®ç¼ºå°‘ '{col}' åˆ—ã€‚å½“å‰åˆ—å: {df.columns.tolist()}")

        # 4. æ ¼å¼è½¬æ¢
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True) # ç¡®ä¿æŒ‰æ—¶é—´æ­£åº (æ—§->æ–°)
        
        # 5. ç¡®ä¿æ˜¯æ•°å€¼ç±»å‹ (AkShare æœ‰æ—¶è¿”å›å­—ç¬¦ä¸²)
        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                
        return df

    @staticmethod
    def download_us_stock(symbol: str = "NVDA", days: int = 365) -> str:
        """
        [ä¸‹è½½å™¨] è·å–ç¾è‚¡æ•°æ®ï¼Œæ¸…æ´—åä¿å­˜åˆ°æœ¬åœ°ã€‚
        è¿”å›: ä¿å­˜çš„ CSV è·¯å¾„
        """
        print(f"[DataProcessor] æ­£åœ¨é€šè¿‡ AkShare ä¸‹è½½ {symbol} æ•°æ®...")
        if not os.path.exists(OUTPUT_DIR):
            os.makedirs(OUTPUT_DIR)
        try:
            # è°ƒç”¨ AkShare æ¥å£ (adjust="qfq" ä»£è¡¨å‰å¤æƒï¼Œé€‚åˆåšæŠ€æœ¯åˆ†æ)
            # æ³¨æ„ï¼šak.stock_us_daily å¯èƒ½ä¼šæ¯”è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…
            df = ak.stock_us_daily(symbol=symbol, adjust="qfq")
            
            # --- æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ– ---
            df_clean = DataProcessor.normalize_akshare_data(df)
            
            # --- æ—¶é—´åˆ‡ç‰‡ (åªå–æœ€è¿‘ N å¤©) ---
            start_date = datetime.now() - timedelta(days=days)
            df_clean = df_clean[df_clean.index >= start_date]
            
            if df_clean.empty:
                raise ValueError(f"ä¸‹è½½æˆåŠŸä½†æ•°æ®ä¸ºç©º (å¯èƒ½æ˜¯æ—¶é—´èŒƒå›´ {days} å¤©å†…æ— æ•°æ®)")

            # ä¿å­˜ä¸ºæ ‡å‡† CSV
            file_path = f"{OUTPUT_DIR}/{symbol}_raw.csv"
            df_clean.to_csv(file_path)
            print(f"[DataProcessor] ä¸‹è½½å¹¶æ¸…æ´—å®Œæˆ: {file_path} (åŒ…å« {len(df_clean)} è¡Œ)")
            
            # --- [ä¿®å¤ç‚¹] è¿”å›æ ‡å‡†å­—å…¸ ---
            return {
                "status": "success",
                "summary": f"æ•°æ®ä¸‹è½½æˆåŠŸã€‚å·²ä¿å­˜è‡³ {file_path}ï¼ŒåŒ…å« {len(df_clean)} è¡Œæ•°æ®ã€‚",
                "processed_path": file_path, # å…³é”®ï¼šæŠŠè·¯å¾„ä¼ å›å»
                "images": []
            }
            
        except Exception as e:
            print(f"[Error] ä¸‹è½½å¤±è´¥: {e}")
            # --- [ä¿®å¤ç‚¹] è¿”å›æ ‡å‡†é”™è¯¯å­—å…¸ ---
            return {"status": "error", "error": str(e)}
            
        except Exception as e:
            print(f"[Error] ä¸‹è½½å¤±è´¥: {e}")
            return None

    @staticmethod
    def add_technical_features(df_path: str):
        """
        [ç‰¹å¾å·¥ç¨‹] è¯»å–æ¸…æ´—å¥½çš„ CSVï¼Œè®¡ç®—æŒ‡æ ‡ã€‚
        """
        try:
            if not df_path or not os.path.exists(df_path):
                return {"status": "error", "error": "æ–‡ä»¶è·¯å¾„æ— æ•ˆ"}

            # è¯»å–æ•°æ® (å› ä¸ºä¹‹å‰å·²ç» normalize è¿‡äº†ï¼Œè¿™é‡Œè¯»å‡ºæ¥å°±æ˜¯æ ‡å‡†çš„)
            df = pd.read_csv(df_path, index_col='Date', parse_dates=True)
            
            # --- è®¡ç®—æŒ‡æ ‡ ---
            
            # 1. MA
            df['MA5'] = df['Close'].rolling(window=5).mean()
            df['MA20'] = df['Close'].rolling(window=20).mean()

            # 2. RSI (14)
            delta = df['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['RSI'] = 100 - (100 / (1 + rs))

            # 3. MACD
            ema12 = df['Close'].ewm(span=12, adjust=False).mean()
            ema26 = df['Close'].ewm(span=26, adjust=False).mean()
            df['MACD'] = ema12 - ema26
            df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
            df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']

            # 4. å¸ƒæ—å¸¦
            std20 = df['Close'].rolling(window=20).std()
            df['Boll_Upper'] = df['MA20'] + 2 * std20
            df['Boll_Lower'] = df['MA20'] - 2 * std20
            df['Boll_Width'] = (df['Boll_Upper'] - df['Boll_Lower']) / df['MA20']

            # --- ä¿å­˜ç»“æœ ---
            df.dropna(inplace=True) # å»é™¤è®¡ç®—äº§ç”Ÿçš„ç©ºå€¼
            
            # æ„é€ è¾“å‡ºæ–‡ä»¶å (ä¾‹å¦‚ NVDA_raw.csv -> NVDA_processed.csv)
            base_name = os.path.basename(df_path).replace("_raw.csv", "")
            new_path = f"{OUTPUT_DIR}/{base_name}_processed.csv"
            df.to_csv(new_path)
            
            return {
                "status": "success",
                "summary": f"ç‰¹å¾å·¥ç¨‹å®Œæˆã€‚è®¡ç®—äº† RSI(æœ€æ–°:{df['RSI'].iloc[-1]:.2f}), MACD ç­‰æŒ‡æ ‡ã€‚",
                "processed_path": new_path,
                "images": []
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from datetime import datetime
from scipy import stats

# ã€é‡è¦ã€‘é˜²æ­¢å¼¹çª—
import matplotlib
matplotlib.use('Agg')

OUTPUT_DIR = "./output"

class RiskEvaluator:
    """Risk Evaluator Class - Statistical and Risk Analysis"""
    
    @staticmethod
    def run_monte_carlo(df_path: str, n_simulations=1000, days=30):
        """
        Monte Carlo Simulation - Simulate future price paths
        
        Args:
            df_path: CSV file path
            n_simulations: Number of simulations, default 1000
            days: Forecast days, default 30
            
        Returns:
            dict: Dictionary containing status, summary and image paths
        """
        try:
            # 1. Load data
            df = pd.read_csv(df_path, parse_dates=['Date'], index_col='Date')
            df.sort_index(inplace=True)
            
            # 2. Core logic - Monte Carlo simulation
            # Calculate log returns
            df['Returns'] = np.log(df['Close'] / df['Close'].shift(1))
            df = df.dropna()
            
            if len(df) < 30:
                return {
                    "status": "error",
                    "error": f"Insufficient data, at least 30 trading days required"
                }
            
            current_price = df['Close'].iloc[-1]
            returns = df['Returns'].values
            
            # Calculate statistical parameters
            mean_return = returns.mean()
            std_return = returns.std()
            
            # Execute simulation
            simulations = np.zeros((n_simulations, days))
            np.random.seed(42)  # Fixed random seed
            
            for i in range(n_simulations):
                random_returns = np.random.normal(mean_return, std_return, days)
                price_path = current_price * np.exp(np.cumsum(random_returns))
                simulations[i] = price_path
            
            # Calculate risk metrics
            final_prices = simulations[:, -1]
            
            # VaR calculation (95% confidence)
            confidence_level = 0.95
            var_95 = current_price - np.percentile(final_prices, 100 * (1 - confidence_level))
            var_percentage = (var_95 / current_price) * 100
            
            # Confidence interval
            ci_lower = np.percentile(final_prices, 2.5)
            ci_upper = np.percentile(final_prices, 97.5)
            
            # 3. Plotting (save to OUTPUT_DIR)
            plt.figure(figsize=(12, 8))
            
            # Plot simulation paths (only some to avoid overcrowding)
            for i in range(min(100, n_simulations)):
                plt.plot(range(days), simulations[i], 
                        color='blue', alpha=0.05, linewidth=0.5)
            
            # Plot mean path
            mean_path = np.mean(simulations, axis=0)
            plt.plot(range(days), mean_path, 
                    color='red', linewidth=2, label='Mean Path')
            
            # Plot confidence interval
            lower_bound = np.percentile(simulations, 2.5, axis=0)
            upper_bound = np.percentile(simulations, 97.5, axis=0)
            plt.fill_between(range(days), lower_bound, upper_bound, 
                            color='red', alpha=0.2, label='95% Confidence Interval')
            
            # Current price line
            plt.axhline(y=current_price, color='green', linestyle='--', 
                       linewidth=2, label=f'Current Price: ${current_price:.2f}')
            
            # Chart settings
            plt.title(f'Monte Carlo Simulation - {n_simulations} {days}-Day Price Paths', 
                     fontsize=14, fontweight='bold')
            plt.xlabel('Trading Days')
            plt.ylabel('Price ($)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Add VaR annotation
            plt.text(days*0.7, current_price*0.9, 
                    f'VaR(95%) = -${var_95:.2f}\\n({var_percentage:.2f}%)',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.8))
            
            # Ensure output directory exists
            if not os.path.exists(OUTPUT_DIR):
                os.makedirs(OUTPUT_DIR)
            
            # Save image
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            img_name = f"monte_carlo_{timestamp}.png"
            save_path = f"{OUTPUT_DIR}/{img_name}"
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()  # Must close
            
            # 4. Return standard dictionary
            return {
                "status": "success",
                "summary": (
                    f"Monte Carlo simulation completed. Based on {len(df)} trading days of historical data, "
                    f"simulated {n_simulations} future {days}-day price paths.\\n"
                    f"Risk Analysis Results:\\n"
                    f"â€¢ VaR(95%): In the next {days} days, there's a 95% probability that maximum loss won't exceed ${var_95:.2f} ({var_percentage:.2f}% of current price)\\n"
                    f"â€¢ 95% Confidence Interval: [${ci_lower:.2f}, ${ci_upper:.2f}]\\n"
                    f"â€¢ Historical Volatility: Daily return standard deviation {std_return:.4%}\\n"
                    f"â€¢ Current Price: ${current_price:.2f}"
                ),
                "images": [save_path]
            }
            
        except FileNotFoundError as e:
            return {
                "status": "error",
                "error": f"File not found: {str(e)}"
            }
        except KeyError as e:
            return {
                "status": "error",
                "error": f"Data column missing: {str(e)}. Please ensure data contains 'Date' and 'Close' columns"
            }
        except Exception as e:
            return {
                "status": "error",
                "error": f"Monte Carlo simulation failed: {str(e)}"
            }
    
    @staticmethod
    def run_distribution_test(df_path: str):
        """
        Return Distribution Test - Analyze return distribution characteristics
        
        Args:
            df_path: CSV file path
            
        Returns:
            dict: Dictionary containing status, summary and image paths
        """
        try:
            # 1. Load data
            df = pd.read_csv(df_path, parse_dates=['Date'], index_col='Date')
            df.sort_index(inplace=True)
            
            # 2. Core logic - Distribution test
            # Calculate returns
            df['Returns'] = df['Close'].pct_change()
            df = df.dropna()
            
            if len(df) < 30:
                return {
                    "status": "error",
                    "error": f"Insufficient data, at least 30 trading days required"
                }
            
            returns = df['Returns'].values * 100  # Convert to percentage
            
            # Calculate statistical indicators
            mean_return = returns.mean()
            std_return = returns.std()
            skewness = stats.skew(returns)
            kurtosis = stats.kurtosis(returns)
            
            # Normality test
            jb_stat, jb_pvalue = stats.jarque_bera(returns)
            
            # 3. Plotting (save to OUTPUT_DIR)
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
            
            # Subplot 1: Histogram vs Normal Distribution
            n_bins = min(50, int(np.sqrt(len(returns))))
            ax1.hist(returns, bins=n_bins, density=True, 
                    alpha=0.7, color='blue', edgecolor='black', 
                    label='Actual Distribution')
            
            # Normal distribution curve
            x = np.linspace(returns.min(), returns.max(), 1000)
            normal_pdf = stats.norm.pdf(x, loc=mean_return, scale=std_return)
            ax1.plot(x, normal_pdf, 'r-', linewidth=2, 
                    label=f'Normal Distribution\\nÎ¼={mean_return:.2f}%, Ïƒ={std_return:.2f}%')
            
            ax1.set_title('Return Distribution vs Normal Distribution', fontsize=12, fontweight='bold')
            ax1.set_xlabel('Daily Returns (%)')
            ax1.set_ylabel('Probability Density')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Subplot 2: Q-Q Plot
            stats.probplot(returns, dist="norm", plot=ax2)
            ax2.set_title('Q-Q Plot (Normality Test)', fontsize=12, fontweight='bold')
            ax2.set_xlabel('Theoretical Quantiles')
            ax2.set_ylabel('Sample Quantiles')
            ax2.grid(True, alpha=0.3)
            
            # Add JB test results on Q-Q plot
            ax2.text(0.05, 0.95, f'JB Statistic: {jb_stat:.2f}\\np-value: {jb_pvalue:.4f}',
                    transform=ax2.transAxes, fontsize=10,
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
            
            # 4. Save image
            if not os.path.exists(OUTPUT_DIR):
                os.makedirs(OUTPUT_DIR)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            img_name = f"distribution_test_{timestamp}.png"
            save_path = f"{OUTPUT_DIR}/{img_name}"
            plt.tight_layout()
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()  # Must close
            
            # 5. Analyze distribution characteristics
            skew_analysis = "Right-skewed" if skewness > 0.5 else "Left-skewed" if skewness < -0.5 else "Approximately symmetric"
            kurtosis_analysis = "Leptokurtic (fat-tailed)" if kurtosis > 1 else "Platykurtic (thin-tailed)" if kurtosis < -1 else "Mesokurtic (normal-like)"
            normality_test = "Does not follow normal distribution" if jb_pvalue < 0.05 else "Approximately follows normal distribution"
            
            # 6. Return standard dictionary
            return {
                "status": "success",
                "summary": (
                    f"Return distribution test completed. Based on {len(df)} trading days of historical return data.\\n"
                    f"Key Findings:\\n"
                    f"1. Basic Statistics:\\n"
                    f"   â€¢ Mean Daily Return: {mean_return:.4f}%\\n"
                    f"   â€¢ Volatility (Std Dev): {std_return:.4f}%\\n"
                    f"2. Distribution Characteristics:\\n"
                    f"   â€¢ Skewness: {skewness:.4f} ({skew_analysis})\\n"
                    f"   â€¢ Kurtosis: {kurtosis:.4f} ({kurtosis_analysis})\\n"
                    f"3. Normality Test:\\n"
                    f"   â€¢ Jarque-Bera Statistic: {jb_stat:.2f} (p-value: {jb_pvalue:.4f})\\n"
                    f"   â€¢ Conclusion: {normality_test}\\n"
                    f"\\nAnalysis shows: The asset return distribution is {skew_analysis.lower()}, "
                    f"exhibits {kurtosis_analysis.lower()} characteristics, and {normality_test.lower()}."
                ),
                "images": [save_path]
            }
            
        except FileNotFoundError as e:
            return {
                "status": "error",
                "error": f"File not found: {str(e)}"
            }
        except KeyError as e:
            return {
                "status": "error",
                "error": f"Data column missing: {str(e)}. Please ensure data contains 'Date' and 'Close' columns"
            }
        except Exception as e:
            return {
                "status": "error",
                "error": f"Distribution test failed: {str(e)}"
            }


# tools.py  (Member B - Professional Version)
import os
import time
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import roc_curve, auc

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score,
    roc_auc_score, confusion_matrix, RocCurveDisplay
)
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

OUTPUT_DIR = "./output"
os.makedirs(OUTPUT_DIR, exist_ok=True)


def _ts_path(prefix: str, ext: str = "png") -> str:
    p = os.path.join(OUTPUT_DIR, f"{prefix}_{int(time.time() * 1000)}.{ext}")
    return p.replace("\\", "/")  # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦ï¼Œåˆ©äºLaTeXå’Œè·¨å¹³å°


def _load_processed(df_path: str) -> pd.DataFrame:
    df = pd.read_csv(df_path)
    # ä¼˜å…ˆè¯†åˆ« Date åˆ—
    if "Date" in df.columns:
        df["Date"] = pd.to_datetime(df["Date"])
        df = df.sort_values("Date").set_index("Date")
    else:
        # å…¼å®¹ index æœ¬èº«å°±æ˜¯æ—¥æœŸçš„æƒ…å†µ
        df.index = pd.to_datetime(df.index)
        df = df.sort_index()
    return df


def _safe_clean_numeric(df: pd.DataFrame) -> pd.DataFrame:
    df = df.replace([np.inf, -np.inf], np.nan)
    return df


def _make_binary_label(df: pd.DataFrame, look_ahead: int, ret_threshold: float = 0.0) -> pd.DataFrame:
    """
    ç”ŸæˆäºŒåˆ†ç±»æ ‡ç­¾ï¼š
    future_return = Close(t+look_ahead)/Close(t) - 1
    label = 1 if future_return > ret_threshold else 0
    è‹¥ ret_threshold > 0ï¼Œæ„å‘³ç€è¿‡æ»¤æ‰ä¸€éƒ¨åˆ†å°æ³¢åŠ¨ï¼ˆå¯é€‰ï¼šä¹Ÿå¯ä»¥å‰”é™¤ä¸­é—´æ®µï¼Œä½†äºŒåˆ†ç±»é€šå¸¸ä¸å‰”é™¤ï¼‰
    """
    out = df.copy()
    out["FUTURE_CLOSE"] = out["Close"].shift(-look_ahead)
    out["FUTURE_RET"] = out["FUTURE_CLOSE"] / out["Close"] - 1.0
    out["TARGET"] = (out["FUTURE_RET"] > ret_threshold).astype(int)
    return out


def _plot_feature_importance(feature_cols: List[str], importances: np.ndarray) -> str:
    order = np.argsort(importances)[::-1]
    topk = min(10, len(feature_cols))
    idx = order[:topk]

    plt.figure(figsize=(10, 5))
    plt.bar(range(topk), importances[idx])
    plt.xticks(range(topk), [feature_cols[i] for i in idx], rotation=30, ha="right")
    plt.title("RandomForest Feature Importance (Top)")
    plt.tight_layout()

    img_path = _ts_path("rf_feature_importance")
    plt.savefig(img_path, dpi=160)
    plt.close()
    return img_path


def _plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> str:
    cm = confusion_matrix(y_true, y_pred)
    cm = cm.astype(float)

    # è¡Œå½’ä¸€åŒ–ï¼ˆæŒ‰çœŸå®ç±»åˆ«å½’ä¸€åŒ–ï¼Œä¾¿äºçœ‹å¬å›ï¼‰
    row_sums = cm.sum(axis=1, keepdims=True)
    cm_pct = np.divide(cm, row_sums, out=np.zeros_like(cm), where=row_sums != 0)

    plt.figure(figsize=(6.2, 5.2))
    im = plt.imshow(cm_pct, vmin=0, vmax=1)  # ç”¨ç™¾åˆ†æ¯”åšè‰²é˜¶æ›´ç»Ÿä¸€
    plt.title("Confusion Matrix (Row-normalized)")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.colorbar(im, fraction=0.046, pad=0.04)

    labels = ["Down/0", "Up/1"]
    plt.xticks([0, 1], labels)
    plt.yticks([0, 1], labels)

    # æ ‡æ³¨ï¼šcount + percent
    for i in range(2):
        for j in range(2):
            count = int(cm[i, j])
            pct = cm_pct[i, j]
            # é¢œè‰²å¯¹æ¯”ï¼šæ·±è‰²èƒŒæ™¯ç”¨ç™½å­—
            text_color = "white" if pct > 0.5 else "black"
            plt.text(
                j, i,
                f"{count}\n{pct:.1%}",
                ha="center", va="center",
                color=text_color,
                fontsize=11
            )

    # ç»†ç½‘æ ¼çº¿ï¼ˆçœ‹èµ·æ¥æ›´åƒè¡¨ï¼‰
    plt.gca().set_xticks(np.arange(-.5, 2, 1), minor=True)
    plt.gca().set_yticks(np.arange(-.5, 2, 1), minor=True)
    plt.grid(which="minor", linestyle="-", linewidth=1)
    plt.tick_params(which="minor", bottom=False, left=False)

    plt.tight_layout()
    img_path = _ts_path("rf_confusion_matrix")
    plt.savefig(img_path, dpi=180)
    plt.close()
    return img_path



from sklearn.metrics import roc_curve, auc

def _plot_roc_curve(model, X_test: np.ndarray, y_test: np.ndarray) -> str:
    """
    æ›´ç¨³å®šçš„ROCç»˜å›¾ï¼š
    - è‹¥ y_test åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼šè¾“å‡ºæ¦‚ç‡ç›´æ–¹å›¾æ›¿ä»£ï¼ˆROCä¸å¯å®šä¹‰ï¼‰
    - å¦åˆ™ï¼šç”¨ roc_curve(drop_intermediate=False) + step plotï¼Œé¿å…â€œå€’ä¸‰è§’â€è§‚æ„Ÿ
    """
    y_test = np.asarray(y_test).astype(int)
    unique = np.unique(y_test)

    # å•ç±»ï¼šROC ä¸å­˜åœ¨ï¼Œæ”¹ç”»æ¦‚ç‡ç›´æ–¹å›¾
    if len(unique) < 2:
        prob = model.predict_proba(X_test)[:, 1]
        plt.figure(figsize=(6, 5))
        plt.hist(prob, bins=25, alpha=0.85)
        plt.title("Probability Histogram (ROC unavailable: single-class test set)")
        plt.xlabel("P(Up)")
        plt.ylabel("Count")
        plt.grid(True, linestyle="--", alpha=0.3)
        plt.tight_layout()

        img_path = _ts_path("rf_prob_hist")
        plt.savefig(img_path, dpi=160)
        plt.close()
        return img_path

    prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, prob, drop_intermediate=False)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6, 5))
    plt.step(fpr, tpr, where="post", linewidth=2)
    plt.plot([0, 1], [0, 1], "k--", alpha=0.5)
    plt.title(f"ROC Curve (AUC={roc_auc:.3f})")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.grid(True, linestyle="--", alpha=0.3)
    plt.tight_layout()

    img_path = _ts_path("rf_roc_curve")
    plt.savefig(img_path, dpi=160)
    plt.close()
    return img_path



class PricePredictor:
    @staticmethod
    def run_rf_prediction(
        df_path: str,
        look_ahead: int = 1,
        ret_threshold: float = 0.0,
        n_splits: int = 5
    ) -> Dict:
        """
        - TimeSeriesSplit åšå›æµ‹ç»Ÿè®¡
        - å›¾ï¼šFeature Importance + Confusion Matrix + ROC/æ¦‚ç‡ç›´æ–¹å›¾
        - è¿”å›ä¸¥æ ¼ï¼šstatus/summary/images
        """
        try:
            df = _safe_clean_numeric(_load_processed(df_path))

            if "Close" not in df.columns:
                raise ValueError("processed æ•°æ®ç¼ºå°‘å…³é”®åˆ—: Close")

            candidate_features = [
                "MA5", "MA20", "RSI",
                "MACD", "MACD_Signal", "MACD_Hist",
                "Boll_Width", "Volume"
            ]
            feature_cols = [c for c in candidate_features if c in df.columns]
            if len(feature_cols) < 4:
                raise ValueError(f"å¯ç”¨ç‰¹å¾åˆ—å¤ªå°‘ï¼š{feature_cols}ã€‚è¯·ç¡®è®¤ processed.csv æ˜¯å¦åŒ…å«æŠ€æœ¯æŒ‡æ ‡åˆ—ã€‚")

            df_labeled = _make_binary_label(df, look_ahead=look_ahead, ret_threshold=ret_threshold)
            data = df_labeled[feature_cols + ["TARGET"]].dropna()

            # æ ·æœ¬æ•°ä¸‹é™ï¼šç¡®ä¿è‡³å°‘èƒ½åš 2 æŠ˜åˆ‡åˆ† + ä¸€ç‚¹æµ‹è¯•é›†
            if len(data) < 120:
                raise ValueError(f"æœ‰æ•ˆæ ·æœ¬ä¸è¶³ï¼ˆ{len(data)}ï¼‰ï¼Œå»ºè®®æä¾›æ›´é•¿å†å²çª—å£æˆ–é™ä½ look_ahead/é˜ˆå€¼ã€‚")

            X = data[feature_cols].values
            y = data["TARGET"].values.astype(int)

            # Baselineï¼šå¤šæ•°ç±»å‡†ç¡®ç‡
            p_up = float(np.mean(y))
            baseline = max(p_up, 1.0 - p_up)

            # åŠ¨æ€æŠ˜æ•°ï¼šè‡³å°‘2æŠ˜ï¼Œä¸”ä¸è¶…è¿‡ n_splits
            max_splits_by_data = max(2, len(data) // 60)
            splits = min(n_splits, max_splits_by_data)
            if splits < 2:
                splits = 2  # ä¿é™©
            tscv = TimeSeriesSplit(n_splits=splits)

            cv_metrics = {"acc": [], "bacc": [], "f1": [], "auc": []}

            last_train_idx, last_test_idx = None, None

            # CV å›æµ‹
            for train_idx, test_idx in tscv.split(X):
                last_train_idx, last_test_idx = train_idx, test_idx

                model = RandomForestClassifier(
                    n_estimators=400,
                    random_state=42,
                    min_samples_leaf=2,
                    n_jobs=-1,
                    class_weight="balanced_subsample"
                )
                model.fit(X[train_idx], y[train_idx])

                pred = model.predict(X[test_idx])
                prob = model.predict_proba(X[test_idx])[:, 1]

                cv_metrics["acc"].append(accuracy_score(y[test_idx], pred))
                cv_metrics["bacc"].append(balanced_accuracy_score(y[test_idx], pred))
                cv_metrics["f1"].append(f1_score(y[test_idx], pred, zero_division=0))
                try:
                    cv_metrics["auc"].append(roc_auc_score(y[test_idx], prob))
                except Exception:
                    cv_metrics["auc"].append(np.nan)

            if last_train_idx is None or last_test_idx is None:
                raise ValueError("TimeSeriesSplit æœªèƒ½ç”Ÿæˆæœ‰æ•ˆåˆ‡åˆ†ï¼Œè¯·æ£€æŸ¥æ•°æ®é•¿åº¦ã€‚")

            # ç”¨æœ€åä¸€æŠ˜è®­ç»ƒå¾—åˆ°â€œå±•ç¤ºæ¨¡å‹â€
            X_train, X_test = X[last_train_idx], X[last_test_idx]
            y_train, y_test = y[last_train_idx], y[last_test_idx]

            final_model = RandomForestClassifier(
                n_estimators=400,
                random_state=42,
                min_samples_leaf=2,
                n_jobs=-1,
                class_weight="balanced_subsample"
            )
            final_model.fit(X_train, y_train)
            pred = final_model.predict(X_test)
            prob = final_model.predict_proba(X_test)[:, 1]

            # æœ€åä¸€æŠ˜ç‚¹ä¼°è®¡
            acc = accuracy_score(y_test, pred)
            bacc = balanced_accuracy_score(y_test, pred)
            f1 = f1_score(y_test, pred, zero_division=0)
            try:
                auc_val = roc_auc_score(y_test, prob)
            except Exception:
                auc_val = float("nan")

            # å›¾ï¼šé‡è¦æ€§ + æ··æ·†çŸ©é˜µ + ROC(å®‰å…¨ç‰ˆ/stepç‰ˆ)
            img1 = _plot_feature_importance(feature_cols, final_model.feature_importances_)
            img2 = _plot_confusion_matrix(y_test, pred)
            img3 = _plot_roc_curve(final_model, X_test, y_test)

            # Top ç‰¹å¾
            importances = final_model.feature_importances_
            order = np.argsort(importances)[::-1]
            top5 = ", ".join([f"{feature_cols[i]}({importances[i]:.3f})" for i in order[:5]])

            # CV å‡å€¼Â±std
            def _mean_std(arr):
                arr = np.array(arr, dtype=float)
                return float(np.nanmean(arr)), float(np.nanstd(arr))

            acc_m, acc_s = _mean_std(cv_metrics["acc"])
            bacc_m, bacc_s = _mean_std(cv_metrics["bacc"])
            f1_m, f1_s = _mean_std(cv_metrics["f1"])
            auc_m, auc_s = _mean_std(cv_metrics["auc"])

            summary = (
                f"éšæœºæ£®æ—æ–¹å‘é¢„æµ‹ï¼ˆlook_ahead={look_ahead}, ret_threshold={ret_threshold:.4f}ï¼‰å®Œæˆã€‚"
                f"æ ·æœ¬ä¸Šæ¶¨æ¯”ä¾‹={p_up:.2%}ï¼Œå¤šæ•°ç±»åŸºçº¿Accuracy={baseline:.2%}ã€‚"
                f"æœ€åä¸€æŠ˜ï¼šAccuracy={acc:.2%}, BalancedAcc={bacc:.2%}, F1={f1:.3f}, ROC-AUC={auc_val:.3f}ã€‚"
                f"æ—¶é—´åºåˆ—CV(å‡å€¼Â±æ ‡å‡†å·®)ï¼šAcc={acc_m:.2%}Â±{acc_s:.2%}, "
                f"BAcc={bacc_m:.2%}Â±{bacc_s:.2%}, F1={f1_m:.3f}Â±{f1_s:.3f}, AUC={auc_m:.3f}Â±{auc_s:.3f}ã€‚"
                f"æœ€é‡è¦ç‰¹å¾Top5ï¼š{top5}ã€‚"
            )

            return {"status": "success", "summary": summary, "images": [img1, img2, img3]}

        except Exception as e:
            return {"status": "error", "error": f"run_rf_prediction failed: {str(e)}"}


    @staticmethod
    def run_regression(df_path: str) -> Dict:
        """
        è¶‹åŠ¿å›å½’ï¼ˆç ”æŠ¥å¸¸ç”¨ï¼‰ï¼š
        - LinearRegression å¯¹ Close åšè¶‹åŠ¿æ‹Ÿåˆ
        - è¾“å‡º slope + RÂ² + å›¾
        """
        try:
            df = _safe_clean_numeric(_load_processed(df_path))
            if "Close" not in df.columns:
                raise ValueError("processed æ•°æ®ç¼ºå°‘ Close åˆ—ï¼Œæ— æ³•å›å½’ã€‚")

            data = df[["Close"]].dropna()
            if len(data) < 60:
                raise ValueError(f"æœ‰æ•ˆæ ·æœ¬å¤ªå°‘ï¼ˆ{len(data)}ï¼‰ï¼Œå»ºè®®è‡³å°‘60æ¡ã€‚")

            x = np.arange(len(data)).reshape(-1, 1)
            y = data["Close"].values

            model = LinearRegression()
            model.fit(x, y)
            y_hat = model.predict(x)

            r2 = r2_score(y, y_hat)
            slope = float(model.coef_[0])

            # ç”»å›¾ï¼šæ•£ç‚¹ + æ‹Ÿåˆçº¿
            plt.figure(figsize=(10, 5))
            plt.scatter(data.index, y, s=10, alpha=0.6)
            plt.plot(data.index, y_hat, linewidth=2)
            plt.title("Close Price Trend (Linear Regression)")
            plt.xticks(rotation=30)
            plt.tight_layout()

            img = _ts_path("price_regression")
            plt.savefig(img, dpi=160)
            plt.close()

            trend = "ä¸Šå‡" if slope > 0 else "ä¸‹é™" if slope < 0 else "æ¨ªç›˜"
            summary = f"å›å½’æ‹Ÿåˆå®Œæˆï¼šè¶‹åŠ¿ä¸ºã€Œ{trend}ã€ï¼ˆslope={slope:.6f}ï¼‰ï¼ŒRÂ²={r2:.3f}ã€‚å·²ç”Ÿæˆè¶‹åŠ¿æ‹Ÿåˆå›¾ã€‚"

            return {"status": "success", "summary": summary, "images": [img]}

        except Exception as e:
            return {"status": "error", "error": f"run_regression failed: {str(e)}"}


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from datetime import datetime
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.seasonal import seasonal_decompose
import warnings

warnings.filterwarnings('ignore')

# ã€é‡è¦ã€‘é˜²æ­¢å¼¹çª—
import matplotlib

matplotlib.use('Agg')

OUTPUT_DIR = "./output"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ã€ä¿®æ”¹ç‚¹ 1ã€‘æ›´æ”¹å­—ä½“è®¾ç½®
# ç§»é™¤ SimHeiï¼Œä½¿ç”¨æ ‡å‡†å­—ä½“ä»¥æ”¯æŒè‹±æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

class MarketRegime:
    @staticmethod
    def run_kmeans_regime(df_path: str, n_clusters=3):
        """
        æ‰§è¡ŒK-Meanså¸‚åœºçŠ¶æ€èšç±»åˆ†æ
        """
        try:
            # 1. åŠ è½½æ•°æ®
            df = pd.read_csv(df_path, index_col=0, parse_dates=True)

            # 2. è®¡ç®—æ”¶ç›Šç‡å’Œæ³¢åŠ¨ç‡
            df['Return'] = df['Close'].pct_change()
            df['Volatility'] = df['Return'].rolling(window=20).std() * np.sqrt(252)

            # ç§»é™¤NaNå€¼
            features = df[['Return', 'Volatility']].dropna()

            # 3. æ ‡å‡†åŒ–ç‰¹å¾
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(features)

            # 4. K-Meansèšç±»
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_scaled)

            # 5. åˆ†æç»“æœ
            df_features = features.copy()
            df_features['Cluster'] = labels
            df_features['Cluster'] = df_features['Cluster'].astype(int)

            # è®¡ç®—æ¯ä¸ªèšç±»çš„ç»Ÿè®¡ä¿¡æ¯
            cluster_stats = {}
            for i in range(n_clusters):
                cluster_data = df_features[df_features['Cluster'] == i]
                cluster_stats[i] = {
                    'mean_return': cluster_data['Return'].mean() * 100,
                    'mean_volatility': cluster_data['Volatility'].mean(),
                    'days_count': len(cluster_data),
                    'frequency': len(cluster_data) / len(df_features) * 100
                }

            # 6. ç¡®å®šå½“å‰çŠ¶æ€ï¼ˆæœ€è¿‘ä¸€å¤©ï¼‰
            current_return = df_features['Return'].iloc[-1] * 100
            current_volatility = df_features['Volatility'].iloc[-1]
            current_cluster = df_features['Cluster'].iloc[-1]

            # ã€ä¿®æ”¹ç‚¹ 2ã€‘å°†èšç±»å‘½åé€»è¾‘æ”¹ä¸ºè‹±æ–‡ï¼Œä»¥ä¾¿å›¾ä¾‹æ˜¾ç¤ºä¸ºè‹±æ–‡
            cluster_names = {}
            for i, stats in cluster_stats.items():
                return_val = stats['mean_return']
                vol_val = stats['mean_volatility']

                if vol_val > 2.5:
                    if return_val < -1:
                        cluster_names[i] = "High Vol Bear"  # é«˜æ³¢åŠ¨å¤§è·Œ
                    elif return_val > 2:
                        cluster_names[i] = "High Vol Bull"  # é«˜æ³¢åŠ¨å¤§æ¶¨
                    else:
                        cluster_names[i] = "High Vol Chop"  # é«˜æ³¢åŠ¨éœ‡è¡
                else:
                    if return_val < -0.5:
                        cluster_names[i] = "Low Vol Bear"   # ä½æ³¢åŠ¨ä¸‹è·Œ
                    elif return_val > 0.5:
                        cluster_names[i] = "Low Vol Bull"   # ä½æ³¢åŠ¨ä¸Šæ¶¨
                    else:
                        cluster_names[i] = "Neutral/Calm"   # ä¸­æ³¢åŠ¨æ¸©å’Œ

            current_state_name = cluster_names.get(current_cluster, "Unknown")

            # 7. ç»˜åˆ¶æ•£ç‚¹å›¾
            plt.figure(figsize=(12, 8))

            colors = ['red', 'green', 'blue', 'orange', 'purple'][:n_clusters]
            for i in range(n_clusters):
                cluster_data = df_features[df_features['Cluster'] == i]
                # ã€ä¿®æ”¹ç‚¹ 3ã€‘Labelæ”¹ä¸ºè‹±æ–‡æ ¼å¼
                plt.scatter(cluster_data['Volatility'], cluster_data['Return'] * 100,
                            c=colors[i], alpha=0.6, s=50,
                            label=f'Regime {i}: {cluster_names.get(i, "Unknown")}')

            # æ ‡è®°å½“å‰ç‚¹
            plt.scatter(current_volatility, current_return,
                        c='black', s=200, marker='*', edgecolors='yellow',
                        label=f'Current ({current_state_name})')

            # ã€ä¿®æ”¹ç‚¹ 4ã€‘åæ ‡è½´å’Œæ ‡é¢˜æ”¹ä¸ºè‹±æ–‡
            plt.xlabel('Volatility (Annualized)', fontsize=12)
            plt.ylabel('Return (%)', fontsize=12)
            plt.title('Market Regime Analysis (K-Means)', fontsize=14, fontweight='bold')
            plt.grid(True, alpha=0.3)
            plt.legend()

            # ä¿å­˜å›¾ç‰‡
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            img_name = f"kmeans_regime_{timestamp}.png"
            save_path = f"{OUTPUT_DIR}/{img_name}"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()

            # 8. ç”Ÿæˆåˆ†ææ‘˜è¦ (ä¿æŒä¸­æ–‡ä¾¿äºé˜…è¯»ï¼Œä½†çŠ¶æ€åä¼šæ˜¾ç¤ºä¸ºè‹±æ–‡)
            analysis_period = f"{df.index[0].date()} è‡³ {df.index[-1].date()}"

            summary = f"""
K-Meanså¸‚åœºçŠ¶æ€èšç±»åˆ†ææŠ¥å‘Š
==================================================
åˆ†æå‘¨æœŸ: {analysis_period}
èšç±»æ•°é‡: {n_clusters}
å½“å‰çŠ¶æ€: ã€{current_state_name}ã€‘ ({df.index[-1].date()})
å½“å‰ä»·æ ¼: ${df['Close'].iloc[-1]:.2f}
å½“å‰æ”¶ç›Šç‡: {current_return:.2f}%
å½“å‰æ³¢åŠ¨ç‡: {current_volatility:.4f}

ğŸ“Š èšç±»è¯¦æƒ…:
"""
            for i in range(n_clusters):
                stats = cluster_stats[i]
                summary += f"çŠ¶æ€{i} ({cluster_names.get(i, 'æœªçŸ¥')}):\n"
                summary += f"  â€¢ å¹³å‡æ”¶ç›Šç‡: {stats['mean_return']:.2f}%\n"
                summary += f"  â€¢ å¹³å‡æ³¢åŠ¨ç‡: {stats['mean_volatility']:.4f}\n"
                summary += f"  â€¢ æŒç»­å¤©æ•°: {stats['days_count']}å¤©\n"
                summary += f"  â€¢ å‡ºç°é¢‘ç‡: {stats['frequency']:.1f}%\n\n"

            market_mean_return = df_features['Return'].mean() * 100
            market_std_return = df_features['Return'].std() * 100
            market_mean_vol = df_features['Volatility'].mean()
            market_std_vol = df_features['Volatility'].std()

            summary += f"""
ğŸ“ˆ å¸‚åœºç‰¹å¾ç»Ÿè®¡:
â€¢ å¹³å‡æ—¥æ”¶ç›Šç‡: {market_mean_return:.3f}%
â€¢ æ”¶ç›Šç‡æ ‡å‡†å·®: {market_std_return:.3f}%
â€¢ å¹³å‡æ³¢åŠ¨ç‡: {market_mean_vol:.4f}
â€¢ æ³¢åŠ¨ç‡æ ‡å‡†å·®: {market_std_vol:.4f}

"""

            return {
                "status": "success",
                "summary": summary,
                "images": [save_path],
            }

        except Exception as e:
            return {
                "status": "error",
                "error": f"K-Meansèšç±»åˆ†æå¤±è´¥: {str(e)}"
            }


class TimeSeriesMiner:
    @staticmethod
    def run_seasonal_decomposition(df_path: str, period=20):
        """
        æ‰§è¡Œæ—¶é—´åºåˆ—åˆ†è§£åˆ†æ
        """
        try:
            # 1. åŠ è½½æ•°æ®
            df = pd.read_csv(df_path, index_col=0, parse_dates=True)

            # 2. æ—¶é—´åºåˆ—åˆ†è§£
            close_prices = df['Close'].dropna()

            # ä½¿ç”¨åŠ æ³•æ¨¡å‹
            decomposition = seasonal_decompose(close_prices, model='additive', period=period)

            # 3. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            trend = decomposition.trend.dropna()
            recent_trend = trend.iloc[-min(10, len(trend)):]
            slope = np.polyfit(range(len(recent_trend)), recent_trend.values, 1)[0]

            seasonal = decomposition.seasonal.dropna()

            resid = decomposition.resid.dropna()
            resid_std = resid.std()
            recent_resid_std = resid.iloc[-min(30, len(resid)):].std()

            # 4. è®¡ç®—è´¡çŒ®åº¦
            total_variation = np.var(close_prices)
            trend_contrib = np.var(trend) / total_variation * 100 if total_variation > 0 else 0
            seasonal_contrib = np.var(seasonal) / total_variation * 100 if total_variation > 0 else 0
            resid_contrib = np.var(resid) / total_variation * 100 if total_variation > 0 else 0

            # 5. ç”Ÿæˆè¶‹åŠ¿åˆ¤æ–­
            if abs(slope) < 0.1:
                trend_strength = "å¾®å¼±"
                recent_trend_direction = "éœ‡è¡"
            elif abs(slope) < 0.5:
                trend_strength = "æ¸©å’Œ"
                recent_trend_direction = "ä¸‹é™" if slope < 0 else "ä¸Šå‡"
            else:
                trend_strength = "å¼ºçƒˆ"
                recent_trend_direction = "ä¸‹é™" if slope < 0 else "ä¸Šå‡"

            # 6. ç»˜åˆ¶å›¾è¡¨
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # å›¾è¡¨1ï¼šåˆ†è§£å›¾
            fig1 = plt.figure(figsize=(15, 10))

            # ã€ä¿®æ”¹ç‚¹ 5ã€‘åˆ†è§£å›¾çš„æ‰€æœ‰ä¸­æ–‡æ ‡ç­¾æ”¹ä¸ºè‹±æ–‡
            # å­å›¾1ï¼šåŸå§‹åºåˆ—
            ax1 = plt.subplot(411)
            ax1.plot(close_prices.index, close_prices, 'b-', linewidth=1.5)
            ax1.set_ylabel('Price', fontsize=10) # ä»·æ ¼
            ax1.set_title('Time Series Decomposition Analysis', fontsize=14, fontweight='bold')
            ax1.grid(True, alpha=0.3)

            # å­å›¾2ï¼šè¶‹åŠ¿
            ax2 = plt.subplot(412)
            ax2.plot(trend.index, trend, 'g-', linewidth=2)
            ax2.set_ylabel('Trend', fontsize=10) # è¶‹åŠ¿
            ax2.grid(True, alpha=0.3)

            # å­å›¾3ï¼šå‘¨æœŸ
            ax3 = plt.subplot(413)
            ax3.plot(seasonal.index, seasonal, 'r-', linewidth=1)
            ax3.set_ylabel('Seasonal', fontsize=10) # å‘¨æœŸ
            ax3.grid(True, alpha=0.3)

            # å­å›¾4ï¼šæ®‹å·®
            ax4 = plt.subplot(414)
            ax4.plot(resid.index, resid, 'k-', linewidth=0.8, alpha=0.7)
            ax4.axhline(y=2 * resid_std, color='r', linestyle='--', alpha=0.5, label='2Ïƒ')
            ax4.axhline(y=-2 * resid_std, color='r', linestyle='--', alpha=0.5)
            ax4.set_ylabel('Residual', fontsize=10) # æ®‹å·®
            ax4.set_xlabel('Date', fontsize=10)     # æ—¥æœŸ
            ax4.grid(True, alpha=0.3)
            ax4.legend()

            plt.tight_layout()

            # ä¿å­˜åˆ†è§£å›¾
            img1_name = f"seasonal_decomposition_{timestamp}.png"
            save_path1 = f"{OUTPUT_DIR}/{img1_name}"
            plt.savefig(save_path1, dpi=300, bbox_inches='tight')
            plt.close()

            # å›¾è¡¨2ï¼šè´¡çŒ®åº¦é¥¼å›¾
            plt.figure(figsize=(10, 8))
            contributions = [trend_contrib, seasonal_contrib, resid_contrib]
            
            # ã€ä¿®æ”¹ç‚¹ 6ã€‘é¥¼å›¾æ ‡ç­¾æ”¹ä¸ºè‹±æ–‡
            labels = ['Trend', 'Seasonal', 'Residual'] 
            colors = ['#4CAF50', '#2196F3', '#FF9800']

            plt.pie(contributions, labels=labels, colors=colors, autopct='%1.1f%%',
                    startangle=90, wedgeprops={'edgecolor': 'white', 'linewidth': 2})
            plt.title('Component Contribution Analysis', fontsize=14, fontweight='bold')

            img2_name = f"decomposition_contrib_{timestamp}.png"
            save_path2 = f"{OUTPUT_DIR}/{img2_name}"
            plt.savefig(save_path2, dpi=300, bbox_inches='tight')
            plt.close()

            # 7. ç”Ÿæˆåˆ†ææ‘˜è¦ (ä¿æŒä¸­æ–‡)
            analysis_period = f"{close_prices.index[0].date()} è‡³ {close_prices.index[-1].date()}"

            summary = f"""
æ—¶é—´åºåˆ—åˆ†è§£åˆ†ææŠ¥å‘Š
==================================================
åˆ†æå‘¨æœŸ: {analysis_period}
åˆ†è§£å‘¨æœŸ: {period} å¤©
æ¨¡å‹ç±»å‹: åŠ æ³•æ¨¡å‹ (additive)

ğŸ“ˆ è¶‹åŠ¿åˆ†æ:
â€¢ è¶‹åŠ¿å¼ºåº¦: {trend_strength} (è´¡çŒ®åº¦: {trend_contrib:.1f}%)
â€¢ è¿‘æœŸè¶‹åŠ¿: {recent_trend_direction} (æ–œç‡: {slope:.4f})
â€¢ å½“å‰è¶‹åŠ¿å€¼: ${trend.iloc[-1]:.2f}

ğŸ”„ å‘¨æœŸåˆ†æ:
â€¢ å‘¨æœŸç‰¹å¾: {'æ— æ˜æ˜¾å‘¨æœŸæ€§' if seasonal_contrib < 5 else 'æœ‰æ˜æ˜¾å‘¨æœŸæ€§'} (è´¡çŒ®åº¦: {seasonal_contrib:.1f}%)
â€¢ å‘¨æœŸæŒ¯å¹…: {seasonal.max() - seasonal.min():.2f} (èŒƒå›´: {seasonal.min():.2f} åˆ° {seasonal.max():.2f})
â€¢ å½“å‰å‘¨æœŸæ•ˆåº”: {seasonal.iloc[-1]:.2f}

ğŸ“Š æ®‹å·®åˆ†æ:
â€¢ æ³¢åŠ¨ç‰¹å¾: {'æ®‹å·®æ³¢åŠ¨ç¨³å®š' if recent_resid_std < resid_std * 1.2 else 'æ®‹å·®æ³¢åŠ¨å¢åŠ '}
â€¢ æ®‹å·®æ ‡å‡†å·®: {resid_std:.2f}
â€¢ è¿‘æœŸæ ‡å‡†å·®: {recent_resid_std:.2f}
â€¢ å¼‚å¸¸ç‚¹æ•°é‡: {len(resid[abs(resid) > 2 * resid_std])}ä¸ª (> Â±{2 * resid_std:.2f})

ğŸ“‹ åˆ†è§£è´¡çŒ®æ€»ç»“:
â€¢ è¶‹åŠ¿åˆ†é‡è´¡çŒ®: {trend_contrib:.1f}%
â€¢ å‘¨æœŸåˆ†é‡è´¡çŒ®: {seasonal_contrib:.1f}%
â€¢ æ®‹å·®åˆ†é‡è´¡çŒ®: {resid_contrib:.1f}%
â€¢ ä¸»è¦é©±åŠ¨å› ç´ : {['æ®‹å·®', 'å‘¨æœŸ', 'è¶‹åŠ¿'][np.argmax([resid_contrib, seasonal_contrib, trend_contrib])]}

"""

            return {
                "status": "success",
                "summary": summary,
                "images": [save_path1, save_path2],
            }

        except Exception as e:
            return {
                "status": "error",
                "error": f"æ—¶é—´åºåˆ—åˆ†è§£å¤±è´¥: {str(e)}"
            }
